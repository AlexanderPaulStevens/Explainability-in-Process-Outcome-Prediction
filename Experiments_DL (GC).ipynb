{"cells":[{"cell_type":"markdown","metadata":{"id":"v0yRQ_ZSMaFG"},"source":["# **Import packages and functions**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669394087223,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"VUdXFuf774wi","outputId":"58909411-fa3d-400d-d0d3-818708655fc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Exception reporting mode: Verbose\n"]}],"source":["%xmode Verbose"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYk7_THvBwiV","executionInfo":{"status":"ok","timestamp":1669394117151,"user_tz":-60,"elapsed":29932,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"}},"outputId":"e4ec1c82-724d-480f-ba09-737be93b48b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CurrentWork/Evaluation-Metrics-and-Guidelines-for-Process-Outcome-Prediction-main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOo4JVUeCQT0","executionInfo":{"status":"ok","timestamp":1669394117152,"user_tz":-60,"elapsed":9,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"}},"outputId":"a9d8d404-80a3-4ce7-f812-12d3f642459a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CurrentWork/Evaluation-Metrics-and-Guidelines-for-Process-Outcome-Prediction-main\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6068,"status":"ok","timestamp":1669394123216,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"uwBik9AUO-VZ"},"outputs":[],"source":["# functions and packages\n","import pandas as pd\n","import numpy as np\n","import os\n","import pickle\n","import random\n","from scipy.stats import spearmanr\n","from sklearn.metrics import roc_auc_score\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from pandas.api.types import is_string_dtype\n","from collections import OrderedDict\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","from scipy.spatial import distance\n","\n","#LSTM\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Dropout, Input, Multiply, concatenate, Embedding, LSTM\n","from tensorflow.keras.layers import Bidirectional, TimeDistributed\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Nadam, Adam, SGD, RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","import tensorflow.keras.utils as ku\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.layers import Softmax, Lambda\n","from tensorflow.keras import backend\n","\n","#CNN\n","from tensorflow.keras.layers import Conv1D\n","\n","#packages from https://github.com/irhete/predictive-monitoring-benchmark/blob/master/experiments/experiments.py\n","import EncoderFactory\n","from DatasetManager import DatasetManager\n","\n","from sklearn import metrics"]},{"cell_type":"markdown","metadata":{"id":"iSXBB6FWOyRN"},"source":["## **Own created functions**"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669394123217,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"uH3LRIBHOyap"},"outputs":[],"source":["#functions\n","#https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2\n","class ColumnEncoder(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        self.columns = None\n","        self.maps = dict()\n","\n","    def transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            # encode value x of col via dict entry self.maps[col][x]+1 if present, otherwise 0\n","            X_copy.loc[:,col] = X_copy.loc[:,col].apply(lambda x: self.maps[col].get(x, -1)+1)\n","        return X_copy\n","\n","    def inverse_transform(self, X):\n","        X_copy = X.copy()\n","        for col in self.columns:\n","            values = list(self.maps[col].keys())\n","            # find value in ordered list and map out of range values to None\n","            X_copy.loc[:,col] = [values[i-1] if 0<i<=len(values) else None for i in X_copy[col]]\n","        return X_copy\n","\n","    def fit(self, X, y=None):\n","        # only apply to string type columns\n","        self.columns = [col for col in X.columns if is_string_dtype(X[col])]\n","        for col in self.columns:\n","            self.maps[col] = OrderedDict({value: num for num, value in enumerate(sorted(set(X[col])))})\n","        return self\n","\n","def prepare_inputs(X_train, X_test, data):  \n","    global ce\n","    ce = ColumnEncoder()\n","    X_train, X_test = X_train.astype(str), X_test.astype(str)\n","    X_train_enc = ce.fit_transform(X_train)\n","    X_test_enc = ce.transform(X_test)\n","    return X_train_enc, X_test_enc\n","    \n","def numeric_padding(sequences, maxlen=None, value=0):\n","    num_samples = len(sequences)\n","    sample_shape = np.asarray(sequences[0]).shape[1:]\n","    x = np.full((num_samples, maxlen) + sample_shape, value)\n","    for idx, s in enumerate(sequences):\n","        trunc = s[:maxlen]\n","        x[idx, :maxlen] = trunc[0]\n","        \n","def create_index(log_df, column):\n","    \"\"\"Creates an idx for a categorical attribute.\n","    Args:\n","        log_df: dataframe.\n","        column: column name.\n","    Returns:\n","        index of a categorical attribute pairs.\n","    \"\"\"\n","    temp_list = log_df[[column]].values.tolist()\n","    subsec_set = {str((x[0])) for x in temp_list}\n","    subsec_set = sorted(list(subsec_set))\n","    alias = dict()\n","    for i, _ in enumerate(subsec_set):\n","        alias[subsec_set[i]] = i + 1\n","    return alias\n","\n","def groupby_caseID(data, cols):\n","    ans = [pd.DataFrame(y) for x, y in data[cols].groupby('Case ID', as_index=False)]\n","    return ans\n","\n","def remove_punctuations(columns_before):\n","    columns = []\n","    for string in columns_before:\n","        new_string = string.replace(\":\", \"_\")\n","        columns.append(new_string)\n","    return columns\n","\n","#call this function with the name of the right column\n","def create_indexes(i, data):\n","    cat_index = create_index(data, i)\n","    cat_index['Start'] = 0\n","    cat_index['End'] = len(cat_index)\n","    index_cat = {v: k for k, v in cat_index.items()}\n","    cat_weights = ku.to_categorical(sorted(index_cat.keys()), len(cat_index))\n","    no_cols = len(data.groupby([i]))+1\n","    return cat_weights, index_cat, cat_index, no_cols\n","\n","def labels_after_grouping(data_train,data_test):\n","    train_labels = []\n","    for i in range (0,len(data_train)):\n","        temp_label = data_train[i]['label'].iloc[0]\n","        train_labels.append(temp_label)\n","\n","    test_labels = []\n","    for i in range (0,len(data_test)):\n","        temp_label = data_test[i]['label'].iloc[0]\n","        test_labels.append(temp_label)\n","    train_y = [1 if i!='regular' else 0 for i in train_labels]\n","    test_y = [1 if i!='regular' else 0 for i in test_labels]\n","    return train_y, test_y\n","\n","def pad_cat_data(cols, data_train, data_test, maxlen):\n","    \n","    #padding of the different categorical columns\n","    #train paddings\n","    paddings_train = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_train)):\n","            temp = []\n","            temp = list(data_train[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_train.append(padded)\n","\n","    #test paddings\n","    paddings_test = []\n","    for i in cols:\n","        padding= []\n","        for k in range(0,len(data_test)):\n","            temp = []\n","            temp = list(data_test[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        #padded = padded/len(data.groupby([i]))\n","        paddings_test.append(padded)\n","    return paddings_train, paddings_test\n","\n","def pad_num_data(cols, data_train, data_test, maxlen, dt_train_prefixes, dt_test_prefixes):\n","    pad_train = []\n","    pad_test  = []\n","    for i in cols:\n","        \n","        padding = []\n","        for k in range(0,len(data_train)):\n","            temp_train = []\n","            temp_train = list(data_train[k][i])\n","            padding.append(temp_train)\n","\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_train_prefixes[i].max() !=0:\n","           \n","            padded = padded/dt_train_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_train.append(padded)\n","   \n","    for i in cols:\n","      \n","        padding = []\n","        for k in range(0,len(data_test)):\n","            temp_test = []\n","            temp_test = list(data_test[k][i])\n","            padding.append(temp_test)\n","      \n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_test.append(padded)\n","    return pad_train, pad_test\n","\n","def reshape_num_data(pad_data, cutoff):\n","        pad_num = np.reshape(pad_data, (len(pad_data), cutoff, 1))\n","        return pad_num"]},{"cell_type":"markdown","metadata":{"id":"i00WpG-jxewE"},"source":["# **Attention**"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1669394130986,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"pI-3_ZdzxiEV"},"outputs":[],"source":["def attention():\n","    #  Generation of predictions\n","    layer_names = [layer.name for layer in model.layers]\n","    print(layer_names)\n","    ac_output_weights, ac_bias = model.get_layer(name='final_output').get_weights()\n","    model_with_attention = Model(model.inputs, model.outputs +\\\n","                                                  [model.get_layer(name='alpha_softmax').output,\\\n","                                                   model.get_layer(name='beta_dense_0').output])\n","    temporal_vectors = []\n","    variable_vectors=[]\n","    predictions = []\n","    for i in range(len(paddings_train[0])):\n","        x_ngram_list = []\n","        for j in range(0,len(paddings_train)):\n","                x_ngram = paddings_train[j][i].reshape((1,cutoff))\n","                x_ngram_list.append(x_ngram)\n","\n","        for k in range(0,len(pad_train)):\n","                x_ngram = np.reshape(pad_train[k], (len(pad_train[k]), cutoff, 1))[i].reshape(1,cutoff,1)\n","                x_ngram_list.append(x_ngram)\n","\n","        #extend list and add time \n","        x_ngram_list.append(padded_time[i].reshape(1,cutoff,1))\n","\n","        proba, alphas, betas = model_with_attention.predict(x_ngram_list)\n","        proba = np.squeeze(proba)\n","        alphas = np.squeeze(alphas)\n","        temporal_att_vec = alphas\n","        assert (np.sum(temporal_att_vec) - 1.0) < 1e-5\n","\n","        #print(temporal_att_vec)\n","        temporal_vectors.append(temporal_att_vec)\n","\n","        #get the beta value\n","        betas = np.squeeze(betas)\n","        idx = np.argmax(alphas)\n","\n","        #print(idx)\n","        beta_val = betas[idx]\n","\n","        dim = 0\n","        emb_list = []\n","        for i in range(0,len(paddings_train)):\n","            ip = int(x_ngram_list[i][0][idx])\n","            i = cat_cols[i]\n","            i = i.replace(':','_')\n","            i = i.replace(' ','_')\n","            emb_weights = model.get_layer(name='embed_'+i).get_weights()[0]\n","            emb = emb_weights[ip]\n","            emb_list.append(emb)\n","            dim += emb.shape[0]\n","\n","\n","        for i in range(0,len(pad_train)):\n","            x_ngram = (reshape_num_data(pad_train[k], cutoff))[i].reshape(1,cutoff,1)\n","            num = np.squeeze(x_ngram)[idx]\n","            emb_list.append(num)\n","            dim +=1\n","        #if cls_method =='LSTM':\n","        if(betas.shape[1]==dim+1):\n","            x_t_ngram = padded_time[i].reshape(1, cutoff, 1)\n","            time_v = np.squeeze(x_t_ngram)[idx]\n","            emb_list.append(time_v)\n","            emb = np.concatenate(tuple(emb_list), axis=None)\n","            #print('beta_val',beta_val.shape)\n","            beta_scaled = np.multiply(beta_val,emb)\n","            variable_attn = alphas[idx] * beta_scaled\n","            variable_vectors.append(variable_attn)\n","            predictions.append(proba)\n","       \n","    if(len(variable_vectors)>0):\n","        var_final = np.mean(np.array(variable_vectors), axis=0)\n","        cat_labels_list = []\n","        current_length = 0\n","        for i in dt_train_prefixes[cat_cols].columns:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            cat_labels = [index_cat[key] for key in sorted(index_cat.keys())]\n","            cat_labels_list.extend(cat_labels)\n","\n","        for i in dt_train_prefixes[numerical_columns].columns:\n","            cat_labels_list.append(i)\n","        cat_labels_list.append('time')\n","        \n","        df_var=pd.DataFrame({'attributes':var_final, 'attribute_values':cat_labels_list})\n","        print(df_var)\n","        df_var.plot.bar(y='attributes', x='attribute_values',\n","                                    title='Attention of the event attributes.', figsize=(10,7))\n","\n","        #plot_history( plt, file_name + 'variable_attn', path )\n","\n","        # Hide grid lines\n","        plt.grid(False)\n","        plt.show()\n","        attention_values = []\n","        for i in dt_train_prefixes[cat_cols].columns:\n","            cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","            new_length = len([index_cat[key] for key in sorted(index_cat.keys())])\n","            attention_value = abs(df_var['attributes'].iloc[current_length:current_length+new_length]).sum(skipna = True)\n","            attention_values.append(attention_value)\n","            current_length += new_length\n","      \n","        for i in dt_train_prefixes[numerical_columns].columns:\n","            attention_values.append(abs(df_var['attributes'].iloc[current_length:current_length+1]).sum(skipna = True))\n","            current_length+1\n","\n","        attention_values.append(abs(df_var['attributes'].iloc[current_length:current_length+1]).sum(skipna = True))\n","\n","    return df_var, attention_values"]},{"cell_type":"markdown","metadata":{"id":"t6qDWGGTxptx"},"source":["# **Parsimony**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhodjI2f_s5F"},"outputs":[],"source":["def var_importance(model, model_inputs, train_y, sample_length):\n","    effects_saved = []\n","    x = data_sample(model_inputs, sample_length)\n","    orig_pred = model.predict(x)\n","    orig_out = metrics.mean_squared_error(train_y, orig_pred)\n","    for i in range(0,len(cat_cols)):  # iterate over the 5 cat features\n","        print(\" \")\n","        new_x = x.copy()\n","        perturbations_list = []\n","        for j in range(0,sample_length):\n","            prefix_array = np.random.random((maxlen,))\n","            perturbations_list.append(prefix_array)\n","        new_x[i] = np.array(perturbations_list)\n","        perturbed_out = model.predict(new_x)\n","        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5\n","        print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n","        effects_saved.append(effect)\n","    for k in range(i+1,len(model_inputs)):  # iterate over the 2 num features\n","        print(k)\n","        new_x = x.copy()\n","        perturbations_list = []\n","        for j in range(0,sample_length):\n","            prefix_array = np.random.random((maxlen,))\n","            perturbations_list.append(prefix_array)\n","        perturbation_array = np.array(perturbations_list)\n","        perturbations_reshaped = perturbation_array.reshape(sample_length, cutoff, 1)\n","        new_x[k] = perturbations_reshaped\n","        perturbed_pred = model.predict(new_x)\n","        perturbed_out = metrics.mean_squared_error(train_y, perturbed_pred)\n","        effect = perturbed_out - orig_out\n","        print('Variable: ',j, 'perturbation effect: ',effect)   \n","        effects_saved.append(effect)\n","    return effects_saved \n","\n","def parsimony(attention_values):\n","  #feature importance of original model\n","  feature_importance=pd.DataFrame()\n","  columns = cat_cols+numerical_columns\n","  columns.append('time')\n","  feature_importance['variable']=columns\n","  feature_importance['coefficients'] = attention_values\n","  \n","  count_event = 0\n","  count_case = 0\n","  count_control = 0\n","\n","  # event columns\n","  model_event = feature_importance[feature_importance['variable'].isin(event_columns)]\n","  model_event = model_event['coefficients'].tolist()\n","  #case columns\n","  model_case= feature_importance[feature_importance['variable'].isin(case_columns)]\n","  model_case = model_case['coefficients'].tolist()\n","  count_case += model_case.count(0.0)\n","  #controlflow columns\n","  model_control= feature_importance[feature_importance['variable'].isin(controlflow_columns)]\n","  model_control = model_control['coefficients'].tolist()\n","  count_control += model_control.count(0.0)\n","  parsimony_event = (len_event-count_event)\n","  parsimony_case = (len_case-count_case)\n","  parsimony_control = (len_control-count_control)\n","  print('parsimony event attributes:', parsimony_event)\n","  print('parsimony case attributes:', parsimony_case)\n","  print('parsimony controlflow attributes', parsimony_control)\n","\n","  return parsimony_event, parsimony_case, parsimony_control\n"]},{"cell_type":"markdown","metadata":{"id":"3m4jrc3axxDj"},"source":["# **Functional complexity**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDT1w0aNYH7R"},"outputs":[],"source":["def create_test_data(dt_test_prefixes):\n","\n","  #cat columns integerencoded\n","  cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols].astype(str)\n","  dt_test_prefixes[cat_cols] = ce.transform(dt_test_prefixes[cat_cols])\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n","  \n","  #append caseId and label\n","  cat_cols.append('Case ID')\n","  cat_cols.append('label')\n","  #groupby case ID\n","  ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n","\n","  #remove then back\n","  cat_cols.remove('label')\n","  cat_cols.remove('Case ID')\n","  \n","  #pad cat columns\n","  paddings_test = []\n","  for i in cat_cols:\n","        padding= []\n","        for k in range(0,len(ans_test)):\n","            temp = []\n","            temp = list(ans_test[k][i])\n","            padding.append(temp)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        paddings_test.append(padded)\n","  \n","  #NUMERICAL COLUMNS\n","  numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","  numerical_columns.remove('timesincelastevent')\n"," \n","  numerical_columns.append('Case ID')\n","  ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n","  numerical_columns.remove('Case ID')\n","  pad_test  = []\n","  \n","  for i in numerical_columns:\n","        padding = []\n","        for k in range(0,len(ans_test2)):\n","            temp_test = []\n","            temp_test = list(ans_test2[k][i])\n","            padding.append(temp_test)\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_test.append(padded)\n","  \n","  #TIME COLUMN\n","  ans_time_test = groupby_caseID(dt_test_prefixes,['timesincelastevent', 'Case ID'])\n","  cols = ['timesincelastevent'] \n","  pad_time_test = []\n","  for i in cols:\n","        padding = []\n","        for k in range(0,len(ans_time_test)):\n","            temp_test = []\n","            temp_test = list(ans_time_test[k][i])\n","            padding.append(temp_test)\n","\n","        padded = np.array(pad_sequences(padding,maxlen=maxlen, padding='pre', truncating='pre',value=0))\n","        if dt_test_prefixes[i].max() !=0:\n","            padded = padded/dt_test_prefixes[i].max()\n","        else:\n","            padded = padded\n","        pad_time_test.append(padded)\n","  \n","  padded_time_test=  reshape_num_data(pad_time_test[0], cutoff)\n","            \n","  return pad_test, paddings_test, padded_time_test\n","\n","def distance_FC(lista, listb):\n","    runsum = 0.0\n","    for a, b in zip(lista, listb):\n","        # square the distance of each\n","        #  then add them back into the sum\n","        runsum += abs(b - a)   \n","\n","    # square root it\n","    return runsum \n","\n","def functional_complexity(test_data, n_instances):\n","    NF_event=0\n","    NF_case=0\n","    NF_control=0\n","    new_model_inputs_test = []\n","    \n","    \n","    #the original prediction, flattened\n","    pred1 = model.predict(model_inputs_test)\n","    \n","    lst3 = []\n","    lst3.extend(pred1)\n","    flat_pred1 = [round(item) for sublist in lst3 for item in sublist]\n","      \n","    ###EVENT COLUMNS###\n","    print('event columns')\n","    result2 = test_data.copy()\n","\n","    for j in event_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n"," \n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","   \n","    pred2 = model.predict(new_model_inputs_test)\n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred2 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_event = distance_FC(flat_pred1, flat_pred2)\n","    print(NF_event, n_instances)\n","    FC_event = NF_event/(n_instances)\n","    print('FC_event: ', FC_event)\n","\n","    ###CASE COLUMNS###\n","    print('case columns')\n","    result2 = test_data.copy()\n","    new_model_inputs_test = []\n","    for j in case_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n","\n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","    pred2 = model.predict(new_model_inputs_test) \n","   \n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred3 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_case = distance_FC(flat_pred1, flat_pred3)\n","    print(NF_case, n_instances)\n","    FC_case = NF_case/(n_instances)\n","    print('FC_case: ', FC_case)\n","    \n","    \n","    #CONTROLFLOW COLUMNS###\n","    print('control columns')\n","    result2 = test_data.copy()\n","    new_model_inputs_test = []\n","\n","    for j in controlflow_columns:\n","        new_items = []\n","        permuted_values  = set(result2[j].values)\n","        for i in range(0,n_instances):\n","            value = result2[j].iloc[i]\n","            permuted_list = np.setdiff1d(list(permuted_values),[value])\n","            if len(permuted_list)<1:\n","                random_value = value\n","            else:\n","                random_value = random.choice(permuted_list)\n","            new_items.append(random_value)\n","        \n","        result2[j] = new_items\n","\n","    new_pad_test, new_paddings_test, new_padded_time_test = create_test_data(result2)\n","        \n","     #add the permuted padded data to the new_model_inputs list\n","    for i in range(0,len(new_paddings_test)):\n","        new_model_inputs_test.append(new_paddings_test[i])\n","\n","    for i in range(0,len(pad_test)):\n","        new_model_inputs_test.append(reshape_num_data(new_pad_test[i], cutoff))\n","    \n","    #add padded time\n","    new_model_inputs_test.append(new_padded_time_test)\n","\n","    pred2 = model.predict(new_model_inputs_test)\n","  \n","    lst3 = []\n","    lst3.extend(pred2)\n","    flat_pred4 = [round(item) for sublist in lst3 for item in sublist]\n","  \n","    \n","    NF_control = distance_FC(flat_pred1, flat_pred4)\n","    print(NF_control, n_instances)\n","    FC_control = NF_control/(n_instances)\n","    print('FC_control: ', FC_control)\n","    \n","    return FC_event, FC_case, FC_control"]},{"cell_type":"markdown","metadata":{"id":"vTmgzrV9x5vk"},"source":["# **Monotonicity**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hk0Mf6jSx54Z"},"outputs":[],"source":["def numbers_list():\n","    numbers = []\n","    for i in cat_cols:\n","        no_cols = len(data.groupby([i]))+1\n","        numbers.append(no_cols)\n","\n","    for i in numerical_columns:\n","        numbers.append(1)\n","\n","    #time layer\n","    numbers.append(1)\n","    return numbers\n","\n","def data_sample(model_inputs, sample_length):\n","    sample_data = []\n","    for i in range(0, len(model_inputs)):\n","        sample_data.append(model_inputs[i][0:sample_length])\n","    return sample_data\n","\n","\n","def monotonicity(attention_values, effects_saved):\n","    len(attention_values)\n","    # prepare data\n","    coef, p = spearmanr(attention_values, effects_saved)\n","    print('lengths', len(attention_values), len(effects_saved))\n","    print('Spearmans correlation coefficient: %.3f' % coef)\n","    # interpret the significance\n","    alpha = 0.05\n","    if p > alpha:\n","        print('Samples are uncorrelated (fail to reject H0) p=%.3f' % p)\n","    else:\n","        print('Samples are correlated (reject H0) p=%.3f' % p)\n","\n","    #feature importance of original model\n","    feature_importance=pd.DataFrame()\n","    columns = cat_cols + numerical_columns\n","    columns.append('time')\n","    feature_importance['columns']=columns\n","    feature_importance['importances'] = effects_saved\n","    attention_importances=pd.DataFrame()\n","    attention_importances['columns']=columns\n","    attention_importances['importances_attention'] = attention_values\n","\n","    #resulting frame\n","    resulting_frame = pd.concat([attention_importances, feature_importance], join='inner', axis=1)\n","    resulting_frame.sort_values(by='importances',ascending=False,inplace=True)\n","\n","     #top 10 of attention values importances and extract which columns are in it\n","    attention_importances.sort_values(by='importances_attention',ascending=False,inplace=True)\n","    attention_top_10 = attention_importances[:10]\n","    #attention_event\n","    attention_event = len(attention_top_10[attention_top_10['columns'].isin(event_columns)])\n","    #attention_case\n","    attention_case = len(attention_top_10[attention_top_10['columns'].isin(case_columns)])\n","    #attention_control\n","    attention_control = len(attention_top_10[attention_top_10['columns'].isin(controlflow_columns)])\n","    \n","    #similar for original model\n","    feature_importance.sort_values(by='importances',ascending=False,inplace=True)\n","    feature_top_10 = feature_importance[:10]\n","    #feature-event\n","    feature_event = len(feature_top_10[feature_top_10['columns'].isin(event_columns)])\n","    #feature-case\n","    feature_case = len(feature_top_10[feature_top_10['columns'].isin(case_columns)])\n","    #feature_control\n","    feature_control = len(feature_top_10[feature_top_10['columns'].isin(controlflow_columns)])\n","\n","    #LOD\n","    model_importance = [feature_event, feature_case, feature_control]\n","    explainability_importance = [attention_event, attention_case, attention_control]\n","    LOD = distance.euclidean(model_importance, explainability_importance)\n","\n","    return coef, LOD"]},{"cell_type":"markdown","metadata":{"id":"Xx3FVtgkOgyq"},"source":["# **parameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cnlhy3XIOgof"},"outputs":[],"source":["# parameters\n","#terminology \n","results_dir = './results__dir_DL' \n","params_dir = './params_dir_DL'\n","column_selection= 'all'\n","cls_encoding ='embeddings'\n","classifiers =['CNN']\n","n_iter = 1\n","n_splits = 3\n","train_ratio = 0.8\n","random_state = 22\n","l2reg = 0.001\n","\n","dataset_ref_to_datasets = {\n","    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(4,5)],\n","    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(1,6)],\n","    \"sepsis_cases\": [\"sepsis_cases_1\",\"sepsis_cases_2\",'sepsis_cases_4'],\n","    \"production\": [\"production\"],\n","}\n","datasets = []\n","for k, v in dataset_ref_to_datasets.items():\n","    datasets.extend(v)\n","\n","allow_negative=False\n","incl_time = True \n","incl_res = True\n","# create results directory\n","if not os.path.exists(os.path.join(results_dir)):\n","    os.makedirs(os.path.join(results_dir))"]},{"cell_type":"markdown","metadata":{"id":"UpCBthWhMhkS"},"source":["# **Function for preprocessing the data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqpKGwadG0sH"},"outputs":[],"source":["def create_data(dt_train_prefixes, dt_test_prefixes):\n","  #get the label of the train and test set\n","  test_y = dataset_manager.get_label_numeric(dt_test_prefixes)\n","  train_y = dataset_manager.get_label_numeric(dt_train_prefixes)   \n","  \n","  #cat columns integerencoded\n","  cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","\n","  dt_train_prefixes[cat_cols],dt_test_prefixes[cat_cols]= prepare_inputs(dt_train_prefixes[cat_cols], dt_test_prefixes[cat_cols], data)\n","  dt_train_prefixes[cat_cols] = dt_train_prefixes[cat_cols]+1\n","  dt_test_prefixes[cat_cols] = dt_test_prefixes[cat_cols]+1\n","  #append caseId and label\n","  cat_cols.append('Case ID')\n","  cat_cols.append('label')\n","  #groupby case ID\n","  \n","  ans_train = groupby_caseID(dt_train_prefixes, cat_cols)\n","  ans_test = groupby_caseID(dt_test_prefixes, cat_cols)\n","  #obtain the new label lists after grouping\n","  train_y, test_y = labels_after_grouping(ans_train, ans_test)\n","  #remove then back\n","  cat_cols.remove('label')\n","  cat_cols.remove('Case ID')\n","  #pad cat columns\n","  paddings_train, paddings_test = pad_cat_data(cat_cols, ans_train, ans_test, maxlen)\n","  \n","  #NUMERICAL COLUMNS\n","  numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","  numerical_columns.remove('timesincelastevent')\n"," \n","  numerical_columns.append('Case ID')\n","  ans_train2 = groupby_caseID(dt_train_prefixes, numerical_columns)\n","  ans_test2 = groupby_caseID(dt_test_prefixes, numerical_columns )\n","  numerical_columns.remove('Case ID')  \n","  pad_train, pad_test = pad_num_data(numerical_columns, ans_train2, ans_test2, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  \n","  #time inputs                   \n","  ans_time_train= groupby_caseID(dt_train_prefixes,['timesincelastevent', 'Case ID'])\n","  ans_time_test = groupby_caseID(dt_test_prefixes,['timesincelastevent', 'Case ID'])\n","  pad_time_train, pad_time_test = pad_num_data(['timesincelastevent'], ans_time_train, ans_time_test, maxlen, dt_train_prefixes, dt_test_prefixes)\n","  #reshape the time input\n","  padded_time = reshape_num_data(pad_time_train[0], cutoff)\n","  padded_time_test=  reshape_num_data(pad_time_test[0], cutoff)\n","            \n","  return pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y"]},{"cell_type":"markdown","metadata":{"id":"PVX1WhP2Mvv-"},"source":["# **loop over datasets and classifiers**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rX07Ni_KeZL08O6IC6tOZd0drsvC8TY8"},"executionInfo":{"elapsed":32300005,"status":"ok","timestamp":1669245093199,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"},"user_tz":-60},"id":"agyg5xxq09yo","outputId":"fb848c45-4502-4365-aae6-48a23a90eab1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for cls_method in classifiers:\n","    for dataset_name in datasets:\n","            print('Dataset:', dataset_name)\n","            print('Classifier', cls_method)\n","            print('Encoding', cls_encoding)\n","            dataset_manager = DatasetManager(dataset_name)\n","            data = dataset_manager.read_dataset() \n","            method_name = \"%s_%s\"%(column_selection,cls_encoding)\n","                     \n","            optimal_params_filename = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n","            if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n","                print('problem')\n","            with open(optimal_params_filename, \"rb\") as fin:\n","                args = pickle.load(fin)\n","                print(args)\n","\n","            cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n","                        'static_cat_cols': dataset_manager.static_cat_cols,\n","                        'static_num_cols': dataset_manager.static_num_cols, \n","                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n","                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n","                        'fillna': True}\n","            \n","            #file to save results\n","            outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))\n","        \n","            # determine min and max (truncated) prefix lengths\n","            min_prefix_length = 1\n","            if \"traffic_fines\" in dataset_name:\n","                max_prefix_length = 10\n","            elif \"bpic2017\" in dataset_name:\n","                max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","            else:\n","                max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n","            maxlen = cutoff = max_prefix_length\n","            \n","            # split into training and test\n","            train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n","            \n","            #prefix generation of train and test data\n","            dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length)\n","            dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n","\n","            pad_train, pad_test, paddings_train, paddings_test, padded_time, padded_time_test, train_y, test_y = create_data(dt_train_prefixes, dt_test_prefixes)\n","            cat_cols = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['static_cat_cols']\n","        \n","            numerical_columns = cls_encoder_args['dynamic_num_cols']+cls_encoder_args['static_num_cols']\n","            numerical_columns.remove('timesincelastevent')\n","\n","            #control flow, event and case \n","            event_columns = cls_encoder_args['dynamic_cat_cols']+cls_encoder_args['dynamic_num_cols']\n","            controlflow_columns =  [x for x in event_columns if 'Activity' in x]\n","            event_columns.remove(controlflow_columns[0])\n","            case_columns = cls_encoder_args['static_cat_cols']+cls_encoder_args['static_num_cols']\n","            len_event = len(event_columns)\n","            len_case = len(case_columns)\n","            len_control = len(controlflow_columns)\n","\n","            #create the input layers and embeddings\n","            embeddings= []\n","            input_layers = []\n","            preds_all = []\n","            nr_events_all = []\n","            test_y_all = []\n","            score = 0\n","            dim = 0\n","            test_y_all = []\n","            test_y_all.extend(test_y)\n","            nr_events = list(dataset_manager.get_prefix_lengths(dt_test_prefixes))\n","            nr_events_all.extend(nr_events)\n","            \n","            for i in cat_cols:\n","                cat_weights, index_cat, cat_index, no_cols = create_indexes(i, data)\n","                i = i.replace(':','_')\n","                i = i.replace(' ','_')\n","                input_layer = Input(shape=(cutoff,), name=i)\n","                embedding = Embedding(cat_weights.shape[0],\n","                                              cat_weights.shape[1],\n","                                              weights=[cat_weights],\n","                                              input_length=no_cols,\n","                                            name='embed_'+i)(input_layer)\n","                embeddings.append(embedding)\n","                input_layers.append(input_layer)\n","                dim += cat_weights.shape[1]\n","\n","            #static input layers\n","\n","            for j in numerical_columns:\n","                j = j.replace('(','_')\n","                j = j.replace(')','_')\n","                j = j.replace(' ','_')\n","                j = j.replace(':','_')\n","                input_layer = Input(shape=(cutoff,1), name=j)\n","                input_layers.append(input_layer)\n","                embeddings.append(input_layer)\n","                dim +=1\n","\n","            #create the model inputs\n","            model_inputs= []\n","            model_inputs_test= []\n","            for i in range(0,len(paddings_train)):\n","                    model_inputs.append(paddings_train[i])\n","\n","            for i in range(0,len(paddings_test)):\n","                    model_inputs_test.append(paddings_test[i])\n","\n","            for i in range(0,len(pad_train)):\n","                    model_inputs.append(reshape_num_data(pad_train[i], cutoff))\n","\n","            for i in range(0,len(pad_test)):\n","                    model_inputs_test.append(reshape_num_data(pad_test[i], cutoff))\n","\n","            model_inputs.append(padded_time)\n","            model_inputs_test.append(padded_time_test)\n","\n","             #Apply dropout on inputs\n","            full_embs = concatenate(embeddings, name='full_embedding')\n","            full_embs = Dropout(args['dropout_rate'])(full_embs)\n","            time_input_layer = Input(shape=(cutoff,1), name='time_input')\n","            input_layers.append(time_input_layer)\n","            time_embs = concatenate([full_embs, time_input_layer], name='allInp')\n","            dim += 1\n","            l2reg=0.001\n","\n","            if cls_method =='LSTM':\n","                \n","                #Compute alpha, visit attention\n","                alpha = Bidirectional(LSTM(args['lstm_size'], return_sequences=True), name='alpha')\n","                alpha_out = alpha(time_embs)\n","                alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n","                alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(alpha_out)\n","                alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n","                \n","                \n","                #Compute beta, codes attention\n","                beta = Bidirectional(LSTM(args['lstm_size'], return_sequences=True),   name='beta')\n","                beta_out = beta(time_embs)\n","                beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n","                beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(beta_out)\n","                \n","                #Compute context vector based on attentions and embeddings\n","                c_t = Multiply()([alpha_out, beta_out, time_embs])\n","                c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n","\n","                #Make a prediction\n","                contexts = Dropout(args['dropout_rate'])(c_t)\n","                output_layer = Dense(1, activation='sigmoid', name='final_output')(contexts)\n","\n","            if cls_method =='CNN':\n","\n","                #conv layer\n","                #input shape of conv1D: (sequences, feature dimension)\n","                #compute alpha\n","                alpha_dense = Dense(1, kernel_regularizer=l2(l2reg))\n","                alpha_out = TimeDistributed(alpha_dense, name='alpha_dense_0')(time_embs)\n","                alpha_out = Softmax(axis=1, name='alpha_softmax')(alpha_out)\n","                \n","                conv1_alpha = Conv1D(filters=time_embs.shape[2], kernel_size= int(time_embs.shape[1]), activation='tanh', input_shape=(maxlen,time_embs.shape[0]))(alpha_out)\n","                #compute beta           \n","                beta_dense = Dense(dim, activation='tanh', kernel_regularizer=l2(l2reg))\n","                beta_out = TimeDistributed(beta_dense, name='beta_dense_0')(time_embs) \n","                conv1_beta = Conv1D(filters=time_embs.shape[2], kernel_size=int(time_embs.shape[1]), activation='tanh', input_shape=(maxlen,time_embs.shape[0]))(beta_out)\n","                #alpha_out = Conv1D(filters=args['filters'], kernel_size= int(alpha_out.shape[1]), activation='tanh', input_shape=(maxlen,time_embs.shape[0]))(alpha_out)\n","                #beta_out = Conv1D(filters=args['filters'], kernel_size=int(beta_out.shape[1]), activation='tanh', input_shape=(maxlen,time_embs.shape[0]))(beta_out)\n","              \n","                #Compute context vector based on attentions and embeddings\n","                c_t = Multiply()([conv1_alpha, conv1_beta, time_embs])\n","                c_t = Lambda(lambda x: backend.sum(x, axis=1))(c_t)\n","                print('cont')\n","                #Make a prediction\n","                contexts = Dropout(args['dropout_rate'])(c_t)\n","                dense_layer = Dense(args['filters'], activation='relu', name='dense_output')(contexts)\n","                output_layer = Dense(1, activation='sigmoid', name='final_output')(dense_layer)\n","                \n","             #MODEL\n","            model = Model(inputs=[input_layers], outputs=output_layer)\n","\n","            if args['optimizer']=='RMSprop':\n","                  opt = RMSprop(learning_rate=args['learning_rate'])\n","            if args['optimizer']=='Nadam':\n","                  opt = Nadam(learning_rate=args['learning_rate'])\n","            if args['optimizer']=='Adam':\n","                  opt = Adam(learning_rate=args['learning_rate'])\n","            if args['optimizer']=='SGD':\n","                  opt = SGD(learning_rate=args['learning_rate'])\n","\n","            model.compile(loss={'final_output':'binary_crossentropy'}, optimizer= opt)\n","\n","            model.summary()\n","\n","            early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n","            model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n","            lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n","\n","            result = model.fit(model_inputs,\n","                  np.array(train_y),\n","                  callbacks=[early_stopping, lr_reducer],\n","                  validation_split = 0.1,\n","                  verbose=2, batch_size=args['batch_size'],\n","                  epochs=500)\n","            pred = model.predict(model_inputs_test)\n","            preds_all.extend(pred)\n","            auc_total = roc_auc_score(test_y_all, preds_all)\n","\n","            #columns\n","            #controlflow columns\n","            controlflow_columns = [word for word in cls_encoder_args['dynamic_cat_cols'] if word.startswith('Act')]\n","            \n","            len_control = len(controlflow_columns)\n","            \n","            #event columns\n","            event_columns = cls_encoder_args['dynamic_cat_cols'].copy()\n","            event_columns.remove(controlflow_columns[0])\n","            event_columns.extend(cls_encoder_args['dynamic_num_cols'].copy())\n","            \n","            len_event = len(event_columns)\n","\n","            #case columns\n","            case_columns = cls_encoder_args['static_cat_cols'].copy()\n","            case_columns.extend(cls_encoder_args['static_num_cols'])\n","\n","            len_case = len(case_columns)\n","\n","            #total amount of columns \n","            total_cols = len(event_columns)+len(controlflow_columns)+len(case_columns)\n","\n","            #attention, parsimony, FC and monotonicity\n","            df_var, attention_values = attention() \n","            effects_saved  = var_importance(model, model_inputs, len(model_inputs[0][0]))\n","            parsimony_event, parsimony_case, parsimony_control = parsimony(attention_values)\n","            FC_event, FC_case, FC_control = functional_complexity(dt_test_prefixes, len(dt_test_prefixes))\n","            print('len event: ', len_event,'len_case: ', len_case, 'len_control', len_control)\n","            monotonicity_value, LOD = monotonicity(attention_values, effects_saved)\n","            print('AUC of model', auc_total)\n","            print('monotonicity value:  ', monotonicity_value)\n","            print('LOD value:   ', LOD)\n","            with open(outfile, 'w') as fout:\n","                fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(\"dataset\", \"method\", \"cls\", \"nr_events\", \"metric\", \"score\"))\n","                \n","                dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n","                for nr_events, group in dt_results.groupby(\"nr_events\"):\n","                    if len(set(group.actual)) < 2:\n","                        fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method, nr_events,-1, \"auc\", np.nan))\n","                    else:\n","                        fout.write(\"%s;%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method, nr_events,-1, \"auc\", roc_auc_score(group.actual, group.predicted)))\n","                fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(dataset_name, method_name, cls_method,-1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted)))\n","                fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(\"event columns:\",len_event,\"  case columns:\", len_case, \"  control columns:\", len_control))\n","                \n","                fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(\"parsimony event\",parsimony_event,\"parsimony case\", parsimony_case, \"parsimony control\", parsimony_control))\n","                fout.write(\"%s;%s;%s;%s;%s;%s\\n\"%(\"FC event\",FC_event,\"FC case\", FC_case, \"FC control\", FC_control))\n","                fout.write(\"%s;%s\\n\"%(\"monotonicity\", monotonicity_value))\n","                fout.write(\"%s;%s\\n\"%(\"LOD\", LOD))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4y86PNWo_a5B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669212693667,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alexander Stevens","userId":"06895933540823444481"}},"outputId":"fd438c90-338d-4c71-e654-3888d6159841"},"outputs":[{"output_type":"stream","name":"stdout","text":["hello\n"]}],"source":["print('hello')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vOo9wnWbwc8"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["4SGhW_-Gygr-","UMxd6y3fIACe","v0yRQ_ZSMaFG","yDqFdSCNzg87","iSXBB6FWOyRN","i00WpG-jxewE","t6qDWGGTxptx","3m4jrc3axxDj","vTmgzrV9x5vk"],"machine_shape":"hm","provenance":[{"file_id":"1xVEDFKM5kGpsOGGOiF2LcT2I6uTlp38J","timestamp":1640767980582},{"file_id":"1ifDBBsMYqN51m5Lm-JvSWmcuoVu9AGU3","timestamp":1638807074915}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}